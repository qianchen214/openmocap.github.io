<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>OpenMoCap — Project Page</title>
    <meta name="description"
        content="OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion (ACM MM 2025)." />
    <meta property="og:title" content="OpenMoCap — Project Page" />
    <meta property="og:description" content="OpenMoCap: Robust motion solving under occlusion. ACM MM 2025." />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="./cover.jpg" />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <header class="hero container">
        <h1>OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion</h1>
        <div class="venue">ACM Multimedia 2025</div>

        <div class="authors">
            Chen Qian<sup>1</sup>, Danyang Li<sup>1*</sup>, Xinran Yu<sup>1</sup>, Zheng Yang<sup>1</sup>, Qiang
            Ma<sup>1</sup>
        </div>
        <div class="affils">
            <sup>1</sup>Tsinghua University, Beijing, China · *Corresponding author
        </div>

        <div class="cta">
            <a class="btn" href="https://arxiv.org/abs/2508.12610" target="_blank" rel="noopener">
                <svg id="logomark" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17.732 24.269" width="20"
                    height="20">
                    <g id="tiny">
                        <path
                            d="M573.549,280.916l2.266,2.738,6.674-7.84c.353-.47.52-.717.353-1.117a1.218,1.218,0,0,0-1.061-.748h0a.953.953,0,0,0-.712.262Z"
                            transform="translate(-566.984 -271.548)" fill="#bdb9b4" />
                        <path
                            d="M579.525,282.225l-10.606-10.174a1.413,1.413,0,0,0-.834-.5,1.09,1.09,0,0,0-1.027.66c-.167.4-.047.681.319,1.206l8.44,10.242h0l-6.282,7.716a1.336,1.336,0,0,0-.323,1.3,1.114,1.114,0,0,0,1.04.69A.992.992,0,0,0,571,293l8.519-7.92A1.924,1.924,0,0,0,579.525,282.225Z"
                            transform="translate(-566.984 -271.548)" fill="#b31b1b" />
                        <path
                            d="M584.32,293.912l-8.525-10.275,0,0L573.53,280.9l-1.389,1.254a2.063,2.063,0,0,0,0,2.965l10.812,10.419a.925.925,0,0,0,.742.282,1.039,1.039,0,0,0,.953-.667A1.261,1.261,0,0,0,584.32,293.912Z"
                            transform="translate(-566.984 -271.548)" fill="#bdb9b4" />
                    </g>
                </svg>
                <span>arXiv</span>
            </a>
            <a class="btn" href="./data/" target="_blank" rel="noopener">
                <img src="./icons/hf-logo.png" alt="Hugging Face" width="20" height="20" style="display:inline-block;"/>
                <span>Data</span></a>
            <a class="btn" href="https://github.com/qianchen214/OpenMoCap" target="_blank" rel="noopener">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 98 96" width="20" height="20">
                    <path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 
                        2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 
                        2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 
                        4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 
                        4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 
                        0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 
                        0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 
                        0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 
                        2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 
                        13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 
                        3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 
                        3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 
                        22 75.788 0 48.854 0z" fill="#24292f" />
                </svg>
                <span>Code</span>
            </a>

        </div>
    </header>

    <main class="container">

        <section>
            <h2>A 5-minute supplementary video</h2>
            <figure>
                <video
                src="./videos/viewpointrosetta_cvpr2025.mp4"
                controls
                preload="metadata"
                playsinline
                poster="./figs/intro_poster.jpg">
                </video>
            </figure>
        </section>

        <section>
            <h2>Abstract</h2>
            <p>
                Optical motion capture is a foundational technology driving advancements in cutting-edge fields such as virtual reality and film production. However, system performance suffers severely under large-scale marker occlusions common in real-world applications. An in-depth analysis identifies two primary limitations of current models: (i) the lack of training datasets accurately reflecting realistic marker occlusion patterns, and (ii) the absence of training strategies designed to capture long-range dependencies among markers. To tackle these challenges, we introduce the CMU-Occlu dataset, which incorporates ray tracing techniques to realistically simulate practical marker occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving model designed specifically for robust motion capture in environments with significant occlusions. Leveraging a marker-joint chain inference mechanism, OpenMoCap enables simultaneous optimization and construction of deep constraints between markers and joints. Extensive comparative experiments demonstrate that OpenMoCap consistently outperforms competing methods across diverse scenarios, while the CMU-Occlu dataset opens the door for future studies in robust motion solving. The proposed OpenMoCap is integrated into the MoSen MoCap system for practical deployment.
            </p>
        </section>

        <section>
            <h2>Problem Overview</h2>
            <p>
                Optical motion capture in real-world scenarios often suffers from severe and long-term occlusions. 
    As shown in Fig. 1, even with multiple cameras, certain markers inevitably become invisible due to 
    body self-occlusion or limited viewpoints. This significantly reduces the reliability of existing 
    solvers and highlights the need for occlusion-aware modeling.
            </p>
            <figure>
                <img src="./figs/intro_ray_trace.png" alt="Overview: marker occlusion and motivation" style="max-width:300px; width:100%; margin:0 auto; display:block;"/>
            </figure>
            
            <p>Fig. 2 illustrates how occlusion directly impacts reconstruction quality. When only visible markers 
    are used as input, prior approaches such as RoMo fail to recover plausible poses under 
    challenging occlusions. In contrast, our proposed <strong>OpenMoCap</strong> maintains accurate 
    and consistent reconstructions, demonstrating its robustness against real-world occlusion patterns.</p>
            <figure>
                <img src="./figs/intro_observe.png" alt="Overview: marker occlusion and motivation" style="max-width:800px; width:100%; margin:0 auto; display:block;" />
            </figure>
            
        </section>

        <section>
            <h2>Our Framework</h2>
            <figure>
                <img src="./figs/archi.png" alt="Framework pipeline" />
                <figcaption>Fig. 2: OpenMoCap architecture with Position Solver and Rotation Solver.</figcaption>
            </figure>
            <figure>
                <img src="./figs/chain.png" alt="Marker-joint chain mechanism" style="max-width:600px; width:100%; margin:0 auto; display:block;"/>
                <figcaption>Fig. 3: Marker-Joint Chain Inference capturing long-range dependencies.</figcaption>
            </figure>
        </section>

        <section>
            <h2>Results</h2>
            <div class="grid-2">

                <div class="table-wrap">
                    <table class="styled-table">
                    <thead>
                        <tr>
                        <th></th>
                        <th></th>
                        <th>MoSh++</th>
                        <th>MoCap-Solver</th>
                        <th>Local MoCap</th>
                        <th>RoMo</th>
                        <th>OpenMoCap</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                        <td rowspan="2"><strong>CMU</strong></td>
                        <td>JPE (cm)</td>
                        <td>2.58</td><td>2.56</td><td>0.94</td><td>0.89</td><td><strong>0.41</strong></td>
                        </tr>
                        <tr>
                        <td>JOE (°)</td>
                        <td>9.40</td><td>6.51</td><td>3.59</td><td>3.43</td><td><strong>2.52</strong></td>
                        </tr>
                        <tr>
                        <td rowspan="2"><strong>CMU-Occlu</strong></td>
                        <td>JPE (cm)</td>
                        <td>2.72</td><td>2.95</td><td>1.23</td><td>1.16</td><td><strong>0.46</strong></td>
                        </tr>
                        <tr>
                        <td>JOE (°)</td>
                        <td>9.68</td><td>6.83</td><td>3.80</td><td>3.54</td><td><strong>2.60</strong></td>
                        </tr>
                    </tbody>
                    </table>
                </div>

                <figure>
                    <img src="./figs/realmocap.png" alt="Qualitative results"
                        style="width:100%; display:block;"/>
                </figure>
            </div>

            <figure>
                <video
                src="./videos/visual_1.mp4"
                controls
                preload="metadata"
                playsinline
                poster="./figs/qual_poster.jpg">
                </video>
                <figcaption>Fig. X: Qualitative demo under severe occlusions.</figcaption>
            </figure>
            <figure>
                <video
                src="./videos/visual_2.mp4"
                controls
                preload="metadata"
                playsinline
                poster="./figs/qual_poster_2.jpg">
                </video>
                <figcaption>Fig. X: Qualitative demo under severe occlusions.</figcaption>
            </figure>
        </section>

        <section id="contact">
            <h2>Contact</h2>
            <p>If you have any questions, please feel free to contact us:</p>
            <ul>
                <li><strong>Chen Qian:</strong> chen.cronus.qian@gmail.com</li>
                <li><strong>Danyang Li:</strong> lidanyang1919@gmail.com</li>
                <li><strong>Xinran Yu:</strong> yuxinran0929@126.com</li>
                <li><strong>Zheng Yang:</strong> hmilyyz@gmail.com</li>
                <li><strong>Qiang Ma:</strong> tsinghuamq@gmail.com</li>
            </ul>
        </section>

        <section>
            <h2>BibTeX</h2>
            <div class="bibtex">
                <button class="copy" id="copyBib">Copy</button>
                <pre id="bib">
@article{qian2025openmocap,
title={OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion},
author={Qian, Chen and Li, Danyang and Yu, Xinran and Yang, Zheng and Ma, Qiang},
journal={arXiv preprint arXiv:2508.12610},
year={2025}
}
                </pre>
            </div>
        </section>

        
    </main>

    <script>
        const btn = document.getElementById('copyBib');
        btn?.addEventListener('click', async () => {
            const text = document.getElementById('bib').innerText;
            try {
                await navigator.clipboard.writeText(text);
                btn.textContent = 'Copied!';
                setTimeout(() => (btn.textContent = 'Copy'), 1600);
            } catch (e) {
                btn.textContent = 'Copy failed';
                setTimeout(() => (btn.textContent = 'Copy'), 1600);
            }
        });
    </script>
</body>

</html>