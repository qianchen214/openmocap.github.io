<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>OpenMoCap — Project Page</title>
    <meta name="description"
        content="OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion (ACM MM 2025)." />
    <meta property="og:title" content="OpenMoCap — Project Page" />
    <meta property="og:description" content="OpenMoCap: Robust motion solving under occlusion. ACM MM 2025." />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="./cover.jpg" />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <header class="hero container">
        <h1>OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion</h1>
        <div class="venue">ACM Multimedia 2025</div>

        <div class="authors">
            Chen Qian<sup>1</sup>, Danyang Li<sup>1*</sup>, Xinran Yu<sup>1</sup>, Zheng Yang<sup>1</sup>, Qiang
            Ma<sup>1</sup>
        </div>
        <div class="affils">
            <sup>1</sup>Tsinghua University, Beijing, China · *Corresponding author
        </div>

        <div class="cta">
            <a class="btn" href="https://arxiv.org/abs/2508.12610" target="_blank" rel="noopener">
                <svg id="logomark" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17.732 24.269" width="20"
                    height="20">
                    <g id="tiny">
                        <path
                            d="M573.549,280.916l2.266,2.738,6.674-7.84c.353-.47.52-.717.353-1.117a1.218,1.218,0,0,0-1.061-.748h0a.953.953,0,0,0-.712.262Z"
                            transform="translate(-566.984 -271.548)" fill="#bdb9b4" />
                        <path
                            d="M579.525,282.225l-10.606-10.174a1.413,1.413,0,0,0-.834-.5,1.09,1.09,0,0,0-1.027.66c-.167.4-.047.681.319,1.206l8.44,10.242h0l-6.282,7.716a1.336,1.336,0,0,0-.323,1.3,1.114,1.114,0,0,0,1.04.69A.992.992,0,0,0,571,293l8.519-7.92A1.924,1.924,0,0,0,579.525,282.225Z"
                            transform="translate(-566.984 -271.548)" fill="#b31b1b" />
                        <path
                            d="M584.32,293.912l-8.525-10.275,0,0L573.53,280.9l-1.389,1.254a2.063,2.063,0,0,0,0,2.965l10.812,10.419a.925.925,0,0,0,.742.282,1.039,1.039,0,0,0,.953-.667A1.261,1.261,0,0,0,584.32,293.912Z"
                            transform="translate(-566.984 -271.548)" fill="#bdb9b4" />
                    </g>
                </svg>
                <span>arXiv</span>
            </a>
            <a class="btn" href="./data/" target="_blank" rel="noopener">
                <img src="./icons/hf-logo.png" alt="Hugging Face" width="20" height="20" style="display:inline-block;"/>
                <span>Data</span></a>
            <a class="btn" href="https://github.com/qianchen214/OpenMoCap" target="_blank" rel="noopener">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 98 96" width="20" height="20">
                    <path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 
                        2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 
                        2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 
                        4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 
                        4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 
                        0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 
                        0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 
                        0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 
                        2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 
                        13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 
                        3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 
                        3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 
                        22 75.788 0 48.854 0z" fill="#24292f" />
                </svg>
                <span>Code</span>
            </a>

        </div>
    </header>

    <main class="container">

        <section>
            <h2>A 3-minute introductory video</h2>
            <figure>
                <video
                src="./videos/openmocap_mm25.mp4"
                controls
                preload="metadata"
                playsinline
                poster="./figs/intro_poster.jpg">
                </video>
            </figure>
        </section>

        <section>
            <h2>Abstract</h2>
            <p>
                Optical motion capture is a foundational technology driving advancements in cutting-edge fields such as virtual reality and film production. However, system performance suffers severely under large-scale marker occlusions common in real-world applications. An in-depth analysis identifies two primary limitations of current models: (i) the lack of training datasets accurately reflecting realistic marker occlusion patterns, and (ii) the absence of training strategies designed to capture long-range dependencies among markers. To tackle these challenges, we introduce the <strong>CMU-Occlu dataset</strong>, which incorporates ray tracing techniques to realistically simulate practical marker occlusion patterns. Furthermore, we propose <strong>OpenMoCap</strong>, a novel motion-solving model designed specifically for robust motion capture in environments with significant occlusions. Leveraging a marker-joint chain inference mechanism, OpenMoCap enables simultaneous optimization and construction of deep constraints between markers and joints. Extensive comparative experiments demonstrate that OpenMoCap consistently outperforms competing methods across diverse scenarios, while the CMU-Occlu dataset opens the door for future studies in robust motion solving. The proposed OpenMoCap is integrated into the MoSen MoCap system for practical deployment.
            </p>
        </section>

        <section>
            <h2>Problem Overview</h2>
            <p>
                Optical Motion Capture (MoCap) in real-world scenarios often suffers from severe and long-term marker occlusions. 
                As illustrated in Fig. 1, even with multiple cameras, certain markers inevitably become invisible due to 
                body self-occlusion or limited viewpoints. The absence of these markers can lead to a drastic degradation in the performance of existing solvers, underscoring the need for models that remain robust under real-world occlusions.
            </p>

            <div class="grid-1-2">
                <!-- 左：小图（保持上限 300px，居中） -->
                <figure>
                    <img
                    src="./figs/intro_ray_trace.png"
                    alt="Ray-traced simulation of visible and occluded markers"
                    style="max-width:300px; width:100%; display:block; margin:0 auto;"
                    />
                    <figcaption style="text-align:center; color:#6b7280; font-size:14px; margin-top:6px;">
                    Fig. 1: Marker occlusion in Optical MoCap.
                    </figcaption>
                </figure>

                <!-- 右：大图（填满右列） -->
                <figure>
                    <img
                    src="./figs/intro_observe.png"
                    alt="Comparison under occlusion (RoMo vs. OpenMoCap)"
                    style="width:100%; display:block;"
                    />
                    <figcaption style="text-align:center; color:#6b7280; font-size:14px; margin-top:6px;">
                    Fig. 2: Performance of solvers in a real MoCap scenario.
                    </figcaption>
                </figure>
            </div>

            
        </section>

        <section>
            <h2>Framework</h2>
            <p>
                OpenMoCap is designed to handle real-world marker occlusions through a two-stage architecture. 
                First, a <strong>Position Solver</strong> recovers the locations of both visible and occluded markers and estimates joint positions, 
                powered by our <strong>Marker–Joint Chain Inference Mechanism</strong> that builds long-range dependencies between markers and joints. 
                Then, a <strong>Rotation Solver</strong> takes the refined positions to predict joint rotations using a stacked attention-based model. 
                Together, these components enable OpenMoCap to deliver accurate motion reconstruction even under severe occlusions.
            </p>
            <!-- <figure>
                <img src="./figs/archi.png" alt="Framework pipeline" />
                <figcaption>Fig. 3: Overview of OpenMoCap.</figcaption>
            </figure>
            <figure>
                <img src="./figs/chain.png" alt="Marker-joint chain mechanism" style="max-width:460px; width:100%; margin:0 auto; display:block;"/>
                <figcaption>Fig. 4: Marker-Joint Chain Inference Mechanism.</figcaption>
            </figure> -->
            <div class="grid-b-s">
                <!-- 左：小图（保持上限 300px，居中） -->
                <figure>
                    <img
                    src="./figs/archi.png"
                    style="max-width:900px; width:100%; display:block;"
                    />
                    <figcaption style="text-align:center; color:#6b7280; font-size:14px; margin-top:6px;">
                    Fig. 3: Overview of OpenMoCap.
                    </figcaption>
                </figure>

                <!-- 右：大图（填满右列） -->
                <figure>
                    <img
                    src="./figs/chain.png"
                    style="max-width:300px; width:100%; display:block; margin:0 auto;"
                    />
                    <figcaption style="text-align:center; color:#6b7280; font-size:14px; margin-top:6px;">
                    Fig. 4: Marker-Joint Chain Inference Mechanism.
                    </figcaption>
                </figure>
            </div>
        </section>

        <section>
            <h2>Dataset</h2>
            <p>
                We introduce the <strong>CMU-Occlu dataset</strong>, which incorporates realistic marker occlusion patterns and overcomes the limitations of existing optical MoCap datasets with overly simplistic and unrealistic occlusion assumptions. This dataset can serve as a benchmark for evaluation in the field of optical motion capture solving.
            </p>
            <figure>
                <img src="./figs/dataset.png" alt="dataset comparison" style="width:60%; display:block; margin:auto;"/>
                <figcaption>Fig. 5: Comparison of marker occlusion patterns of different datasets.</figcaption>
            </figure>
        </section>

        <section>
            <h2>Demo</h2>
            <p>
                Below are qualitative results of OpenMoCap.
                Our method robustly reconstructs accurate human poses despite significant marker occlusions, demonstrating its effectiveness and reliability for practical applications.
            </p>
            <figure>
                <video
                src="./videos/visual_1.mp4"
                controls
                preload="metadata"
                playsinline
                poster="./figs/qual_poster.jpg">
                </video>
                <!-- <figcaption>Fig. X: Qualitative demo under severe occlusions.</figcaption> -->
            </figure>
            <figure>
                <video
                src="./videos/visual_2.mp4"
                controls
                preload="metadata"
                playsinline
                poster="./figs/qual_poster_2.jpg">
                </video>
                <!-- <figcaption>Fig. X: Qualitative demo under severe occlusions.</figcaption> -->
            </figure>
            
        </section>

        <section>
            <h2>Comparison & Application</h2>
            <p>
                We conduct experiments on both the CMU and CMU-Occlu datasets to evaluate the performance of OpenMoCap against state-of-the-art methods. 
                Tab.1 reports quantitative results, where OpenMoCap consistently achieves the best performance across all metrics. 
                In addition, we collected real-world MoCap data, where an actor performs a Russian twist, and compared the reconstruction results obtained from different solvers, as illustrated in Fig. 6.
            </p>

            <div class="grid-2">

                <div class="table-wrap">
                    <table class="styled-table">
                        <caption style="caption-side:bottom; text-align:center; font-size:14px; color:#6b7280; margin-top:6px;">
                            Tab. 1: Comparison of MoCap solvers on CMU and CMU-Occlu datasets.
                        </caption>
                    <thead>
                        <tr>
                        <th></th>
                        <th></th>
                        <th>MoSh++</th>
                        <th>MoCap-Solver</th>
                        <th>Local MoCap</th>
                        <th>RoMo</th>
                        <th>OpenMoCap</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                        <td rowspan="2"><strong>CMU</strong></td>
                        <td>JPE (cm)</td>
                        <td>2.58</td><td>2.56</td><td>0.94</td><td>0.89</td><td><strong>0.41</strong></td>
                        </tr>
                        <tr>
                        <td>JOE (°)</td>
                        <td>9.40</td><td>6.51</td><td>3.59</td><td>3.43</td><td><strong>2.52</strong></td>
                        </tr>
                        <tr>
                        <td rowspan="2"><strong>CMU-Occlu</strong></td>
                        <td>JPE (cm)</td>
                        <td>2.72</td><td>2.95</td><td>1.23</td><td>1.16</td><td><strong>0.46</strong></td>
                        </tr>
                        <tr>
                        <td>JOE (°)</td>
                        <td>9.68</td><td>6.83</td><td>3.80</td><td>3.54</td><td><strong>2.60</strong></td>
                        </tr>
                    </tbody>
                    </table>
                </div>

                <figure>
                    <img src="./figs/realmocap.png" alt="Qualitative results"
                        style="width:100%; display:block;"/>
                    <figcaption>Fig. 6: Qualitative results on real-world capture data.</figcaption>
                </figure>
            </div>

            
        </section>

        <section id="contact">
            <h2>Contact</h2>
            <p>If you have any questions, please feel free to contact us:</p>
            <ul>
                <li><strong>Chen Qian:</strong> chen.cronus.qian@gmail.com</li>
                <li><strong>Danyang Li:</strong> lidanyang1919@gmail.com</li>
                <li><strong>Xinran Yu:</strong> yuxinran0929@126.com</li>
                <li><strong>Zheng Yang:</strong> hmilyyz@gmail.com</li>
                <li><strong>Qiang Ma:</strong> tsinghuamq@gmail.com</li>
            </ul>
        </section>

        <section>
            <h2>BibTeX</h2>
            <div class="bibtex">
                <button class="copy" id="copyBib">Copy</button>
                <pre id="bib">
@article{qian2025openmocap,
title={OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion},
author={Qian, Chen and Li, Danyang and Yu, Xinran and Yang, Zheng and Ma, Qiang},
journal={arXiv preprint arXiv:2508.12610},
year={2025}
}
                </pre>
            </div>
        </section>

        
    </main>

    <script>
        const btn = document.getElementById('copyBib');
        btn?.addEventListener('click', async () => {
            const text = document.getElementById('bib').innerText;
            try {
                await navigator.clipboard.writeText(text);
                btn.textContent = 'Copied!';
                setTimeout(() => (btn.textContent = 'Copy'), 1600);
            } catch (e) {
                btn.textContent = 'Copy failed';
                setTimeout(() => (btn.textContent = 'Copy'), 1600);
            }
        });
    </script>
</body>

</html>